{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "import copy \n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import networkx as nx \n",
    "from functools import reduce\n",
    "import gc\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./../Data/\"\n",
    "sub_folder = \"TR_Y/\"\n",
    "\n",
    "TEMPORAL_PRECISION = \"Y\"\n",
    "TEMPORAL_LIMIT_START = np.datetime64(\"1900-01-01\", TEMPORAL_PRECISION)\n",
    "TEMPORAL_LIMIT_END = np.datetime64(\"2023-05-25\", TEMPORAL_PRECISION)\n",
    "TODAY = np.datetime64(\"2023-05-25\", TEMPORAL_PRECISION)\n",
    "\n",
    "DEFAULT_START = TEMPORAL_LIMIT_START\n",
    "DEFAULT_END = TEMPORAL_LIMIT_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time(time_raw:str, time_precision:str, unknown:str=\"Unknown\") -> np.datetime64:\n",
    "\tif time_raw == unknown:\n",
    "\t\treturn unknown\n",
    "\treturn np.datetime64(time_raw, time_precision)\n",
    "\n",
    "def read_quad(file_path:str, time_precision:str, \n",
    "\t\t\t  threshold_start:np.datetime64, threshold_end:np.datetime64,\n",
    "\t\t\t   unknown:str=\"Unknown\") -> set:\n",
    "\tremoved = 0\n",
    "\twith open(file_path, \"r\", encoding=\"UTF-8\") as f_r: \n",
    "\t\tline = f_r.readline()\n",
    "\t\tgraph = set()\n",
    "\t\twhile line != \"\":\n",
    "\t\t\telts = line[:-2].split(\"\\t\") # Remove \\n and .\n",
    "\t\t\ts = elts[0]\n",
    "\n",
    "\t\t\tif s[:len(\"<http://www.wikidata.org/entity/Q\")] == \"<http://www.wikidata.org/entity/Q\":\n",
    "\t\t\t\tp = elts[1]\n",
    "\t\t\t\tis_object_uri = None\n",
    "\t\t\t\tif elts[2][:len(\"<http://www.wikidata.org/entity/Q\")] == \"<http://www.wikidata.org/entity/Q\":\n",
    "\t\t\t\t\to = elts[2]\n",
    "\t\t\t\t\tis_object_uri = True\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfound = re.findall('\"b[0-9]+\"', elts[2])\n",
    "\t\t\t\t\tif (len(found) == 1) and (found[0] == elts[2]):\n",
    "\t\t\t\t\t\to = f\"\"\"<http://www.wikidata.org/entity/Unkonwn_{elts[2][1:-1]}>\"\"\"\n",
    "\t\t\t\t\t\tis_object_uri = True\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\to = \" \".join(elts[2:-1])\n",
    "\t\t\t\t\t\tis_object_uri = False\n",
    "\t\t\t\ttimestamp = process_time(elts[-1], time_precision=time_precision, unknown=unknown)\n",
    "\t\t\t\tif (timestamp == unknown) or ((threshold_start <= timestamp) and (timestamp <= threshold_end)):\n",
    "\t\t\t\t\tgraph.add((s, p, o, timestamp, is_object_uri, True))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tremoved+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tremoved+=1\n",
    "\t\t\tline = f_r.readline()\n",
    "\tprint(f\"Number removed : {removed}\")\n",
    "\treturn graph \n",
    "\n",
    "def read_quint(file_path:str, time_precision:str,  \n",
    "\t\t\t   threshold_start:np.datetime64, threshold_end:np.datetime64, unknown:str=\"Unknown\") -> set:\n",
    "\tremoved = 0\n",
    "\twith open(file_path, \"r\", encoding=\"UTF-8\") as f_r: \n",
    "\t\tline = f_r.readline()\n",
    "\t\tgraph = set()\n",
    "\t\twhile line != \"\":\n",
    "\t\t\telts = line[:-2].split(\"\\t\") # Remove \\n and .\n",
    "\t\t\ts = elts[0]\n",
    "\n",
    "\t\t\tif s[:len(\"<http://www.wikidata.org/entity/Q\")] == \"<http://www.wikidata.org/entity/Q\":\n",
    "\t\t\t\tp = elts[1]\n",
    "\t\t\t\tis_object_uri = None\n",
    "\t\t\t\tif elts[2][:len(\"<http://www.wikidata.org/entity/Q\")] == \"<http://www.wikidata.org/entity/Q\":\n",
    "\t\t\t\t\to = elts[2]\n",
    "\t\t\t\t\tis_object_uri = True\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfound = re.findall('\"b[0-9]+\"', elts[2])\n",
    "\t\t\t\t\tif (len(found) == 1) and (found[0] == elts[2]):\n",
    "\t\t\t\t\t\to = f\"\"\"<http://www.wikidata.org/entity/Unkonwn_{elts[2][1:-1]}>\"\"\"\n",
    "\t\t\t\t\t\tis_object_uri = True\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\to = \" \".join(elts[2:-1])\n",
    "\t\t\t\t\t\tis_object_uri = False\n",
    "\t\t\t\tstart = process_time(elts[-2], time_precision=time_precision, unknown=unknown)\n",
    "\t\t\t\tend = process_time(elts[-1], time_precision=time_precision, unknown=unknown)\n",
    "\t\t\t\tif (start != unknown) and (end != unknown):\n",
    "\t\t\t\t\tif start < end:\n",
    "\t\t\t\t\t\ttimestamp = (elts[-2], elts[-1])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttimestamp = (elts[-1], elts[-2])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttimestamp = (elts[-2], elts[-1])\n",
    "\n",
    "\t\t\t\tif ((start == unknown) or (start >= threshold_start and start <= threshold_end))\\\n",
    "\t\t\t\t\tand ((end == unknown) or (end >= threshold_start and end <= threshold_end)):\n",
    "\t\t\t\t\tgraph.add((s, p, o, timestamp, is_object_uri, False))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tremoved+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tremoved+=1\n",
    "\t\t\tline = f_r.readline()\n",
    "\tprint(f\"Number removed : {removed}\")\n",
    "\treturn graph\n",
    "\n",
    "def indexed_graph(list_of_graph:list): \n",
    "\tindexed_entity = dict()\n",
    "\tfor g in list_of_graph:\n",
    "\t\tfor fact in g:\n",
    "\t\t\ts, p, o, t, is_object_uri, is_quad = fact\n",
    "\n",
    "\t\t\tnamespace_wikidata = \"<http://www.wikidata.org/entity/Q\"\n",
    "\n",
    "\t\t\ts_reduced = s[:-1].split(\"/\")[-1]\n",
    "\n",
    "\t\t\tif s_reduced[0] == \"Q\":\n",
    "\n",
    "\t\t\t\tif (not is_object_uri) or o[:len(namespace_wikidata)]!=namespace_wikidata:\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif s_reduced not in indexed_entity:\n",
    "\t\t\t\t\t\tindexed_entity[s_reduced] = [[set(), set()], set()]\n",
    "\t\t\t\t\tindexed_entity[s_reduced][0][1].add(fact)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\n",
    "\t\t\t\t\to_reduced = o[:-1].split(\"/\")[-1]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif s_reduced not in indexed_entity:\n",
    "\t\t\t\t\t\tindexed_entity[s_reduced] = [[set(), set()], set()]\n",
    "\t\t\t\t\tindexed_entity[s_reduced][0][0].add(fact)\n",
    "\n",
    "\t\t\t\t\tif o_reduced not in indexed_entity:\t\n",
    "\t\t\t\t\t\tindexed_entity[o_reduced] = [[set(), set()], set()]\n",
    "\t\t\t\t\tindexed_entity[o_reduced][1].add(fact)\n",
    "\t\t\t\n",
    "\treturn indexed_entity\n",
    "\n",
    "def return_entities(list_of_graph:list): \n",
    "\tentities = set()\n",
    "\tfor g in list_of_graph:\n",
    "\t\tfor fact in g:\n",
    "\t\t\ts, p, o, t, is_object_uri, is_quad = fact\n",
    "\n",
    "\t\t\tnamespace_wikidata = \"<http://www.wikidata.org/entity/Q\"\n",
    "\n",
    "\t\t\ts_reduced = s[:-1].split(\"/\")[-1]\n",
    "\n",
    "\t\t\tif s_reduced[0] == \"Q\":\n",
    "\n",
    "\t\t\t\tif (not is_object_uri) or o[:len(namespace_wikidata)]!=namespace_wikidata:\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tentities.add(s_reduced)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tentities.add(s_reduced)\n",
    "\n",
    "\t\t\t\t\to_reduced = o[:-1].split(\"/\")[-1]\n",
    "\t\t\t\t\tentities.add(o_reduced)\n",
    "\t\t\t\n",
    "\treturn entities\n",
    "\n",
    "def load_labels(file_path:str) -> dict[str]:\n",
    "\tlabels_per_class = dict()\n",
    "\twith open(file_path, \"r\", encoding=\"UTF-8\") as f_r:\n",
    "\t\tline = f_r.readline()\n",
    "\t\twhile line != \"\":\n",
    "\t\t\tq, label = line[:-1].split(\"\\t\")\n",
    "\t\t\tq = q[:-1].split(\"/\")[-1]\n",
    "\t\t\tlabels_per_class[q] = label\n",
    "\t\t\tline = f_r.readline()\n",
    "\treturn labels_per_class\n",
    "\n",
    "def select_seeds_bottom_percent(threshold:int, \n",
    "\t\t\t\t\t\t\t\tpercentage:float, \n",
    "\t\t\t\t\t\t\t\tn_entities:int, \n",
    "\t\t\t\t\t\t\t\tdf_entities_per_c:pd.DataFrame, \n",
    "\t\t\t\t\t\t\t\tents_per_classes_and_sub_classes:dict):\n",
    "\n",
    "\t# Retrieve the classes \n",
    "\tabove_threshold = df_entities_per_c[df_entities_per_c[\"Count\"] >= threshold]\n",
    "\tvalue_below = above_threshold.iloc[len(above_threshold)-round(len(above_threshold)*percentage)][\"Count\"]\n",
    "\tselected_classes = above_threshold[above_threshold[\"Count\"] <= value_below].index\n",
    "\n",
    "\t# Retrieve the entities per classes \n",
    "\tselected_entities_per_class = dict()\n",
    "\tfor c in selected_classes:\n",
    "\t\tselected_entities_per_class[c] = set(rand.sample(list(ents_per_classes_and_sub_classes[c]), n_entities))\n",
    "\n",
    "\treturn reduce(lambda x, y: x.union(y), [v for v in selected_entities_per_class.values()])\n",
    "\n",
    "def select_seeds_uniformly_abs(threshold, percentage, n_entities, df_entities_per_c, ents_per_classes_and_sub_classes):\n",
    "\n",
    "\t# Retrieve the classes \n",
    "\tabove_threshold = df_entities_per_c[df_entities_per_c[\"Count\"] >= threshold]\n",
    "\tvalue_below = above_threshold.iloc[len(above_threshold)-round(len(above_threshold)*percentage)][\"Count\"]\n",
    "\tselected_classes = above_threshold[above_threshold[\"Count\"] <= value_below].index\n",
    "\n",
    "\t# Retrieve the entities per classes \n",
    "\tselected_entities_per_class = dict()\n",
    "\tfor c in selected_classes:\n",
    "\t\tselected_entities_per_class[c] = set(rand.sample(list(ents_per_classes_and_sub_classes[c]), min(n_entities, df_entities_per_c.loc[c][\"Count\"])))\n",
    "\n",
    "\treturn reduce(lambda x, y: x.union(y), [v for v in selected_entities_per_class.values()])\n",
    "\n",
    "def select_seeds_uniformly_percent_entities(threshold, percentage, p_entities, df_entities_per_c, ents_per_classes_and_sub_classes):\n",
    "\n",
    "\t# Retrieve the classes \n",
    "\tabove_threshold = df_entities_per_c[df_entities_per_c[\"Count\"] >= threshold]\n",
    "\tvalue_below = above_threshold.iloc[len(above_threshold)-round(len(above_threshold)*percentage)][\"Count\"]\n",
    "\tselected_classes = above_threshold[above_threshold[\"Count\"] <= value_below].index\n",
    "\n",
    "\t# Retrieve the entities per classes \n",
    "\tselected_entities_per_class = dict()\n",
    "\tfor c in selected_classes:\n",
    "\t\tselected_entities_per_class[c] = set(rand.sample(list(ents_per_classes_and_sub_classes[c]), max(1, round(df_entities_per_c.loc[c][\"Count\"]*p_entities))))\n",
    "\n",
    "\treturn reduce(lambda x, y: x.union(y), [v for v in selected_entities_per_class.values()])\n",
    "\n",
    "def compute_comparison_post_section_seeds(seeds, ents_per_classes_and_sub_classes):\n",
    "\tpopulation_of_classes_post_selection = {\n",
    "\t\tc:len(ents_per_classes_and_sub_classes[c].intersection(seeds)) for c in ents_per_classes_and_sub_classes\n",
    "\t}\n",
    "\n",
    "\tcomparison = pd.DataFrame.from_dict(population_of_classes_post_selection, orient=\"index\", columns=[\"Population\"]).sort_values(by=\"Population\", ascending=False).merge(df_entities_per_c, left_index=True, right_index=True)\n",
    "\tcomparison[\"Percent\"] = comparison.apply(lambda x: x[\"Population\"]/x[\"Count\"]*100 if x[\"Count\"] > 0 else 0, axis=1)\n",
    "\treturn comparison\n",
    "\n",
    "def retrieve_next_hops(seeds, already_seen, graph):\n",
    "\textracted_graph = set()\n",
    "\tto_explore = set()\n",
    "\n",
    "\tfor line in graph:\n",
    "\t\ts, _, o, _, is_object_uri, _ = line \n",
    "\t\tif is_object_uri:\n",
    "\t\t\ts = s[:-1].split(\"/\")[-1]\n",
    "\t\t\to = o[:-1].split(\"/\")[-1]\n",
    "\n",
    "\t\t\tif s in seeds or o in seeds:\n",
    "\t\t\t\textracted_graph.add(line)\n",
    "\t\t\t\tif s not in already_seen :\n",
    "\t\t\t\t\tto_explore.add(s)\n",
    "\t\t\t\tif o not in already_seen :\n",
    "\t\t\t\t\tto_explore.add(o)\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\ts = s[:-1].split(\"/\")[-1]\n",
    "\n",
    "\t\t\tif s in seeds:\n",
    "\t\t\t\textracted_graph.add(line)\n",
    "\t\t\t\tif s not in already_seen :\n",
    "\t\t\t\t\tto_explore.add(s)\n",
    "\n",
    "\treturn extracted_graph, to_explore\n",
    "\n",
    "def extract_graph_from_seeds(seeds, graph_to_consider, nb_hops, verbose=1):\n",
    "\n",
    "\talready_seen = set()\n",
    "\textracted_graph = set() \n",
    "\n",
    "\tfor _ in range(nb_hops):\n",
    "\t\textracted_graph_tp, to_explore = retrieve_next_hops(seeds=seeds, already_seen=already_seen, graph=graph_to_consider)\n",
    "\t\textracted_graph.update(extracted_graph_tp)\n",
    "\t\tdel extracted_graph_tp\n",
    "\t\talready_seen.update(seeds)\n",
    "\t\tseeds = to_explore\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(len(extracted_graph)/len(graph_to_consider)*100)\n",
    "\n",
    "\treturn extracted_graph, to_explore, already_seen\n",
    "\n",
    "def find_the_degree_of_nodes_to_explore(to_explore, extracted_graph):\n",
    "\tpopulation_of_to_be_seen_next = {\n",
    "\t\te:0 for e in to_explore\n",
    "\t}\n",
    "\tfor line in extracted_graph:\n",
    "\t\ts, _, o, _, is_object, _ = line \n",
    "\t\ts = s[:-1].split(\"/\")[-1]\n",
    "\n",
    "\t\tif s in population_of_to_be_seen_next:\n",
    "\t\t\tpopulation_of_to_be_seen_next[s]+=1\n",
    "\n",
    "\t\tif is_object:\n",
    "\t\t\to = o[:-1].split(\"/\")[-1]\n",
    "\n",
    "\t\t\tif o in population_of_to_be_seen_next:\n",
    "\t\t\t\tpopulation_of_to_be_seen_next[o]+=1\n",
    "\treturn pd.Series({c:v for c,v in population_of_to_be_seen_next.items() if c[0] == \"Q\"})\n",
    "#vc = pd.Series({c:v for c,v in population_of_to_be_seen_next.items() if c[0] == \"Q\"}).value_counts()\n",
    "#sum(vc.iloc[[i >= 2 for i in vc.index]].values)/sum(vc.values)\n",
    "\n",
    "def find_the_population_of_each_explored_class(already_seen, df_entities_per_c, ents_per_classes_and_sub_classes):\n",
    "\t# Only the fully explored nodes (i.e. to_be_explored) does not count\n",
    "\tpopulation_of_classes_post_extraction = {\n",
    "\t\tc:len(ents_per_classes_and_sub_classes[c].intersection(already_seen)) for c in ents_per_classes_and_sub_classes\n",
    "\t}\n",
    "\n",
    "\tcomparison = pd.DataFrame.from_dict(population_of_classes_post_extraction, orient=\"index\", columns=[\"Population\"]).merge(df_entities_per_c, left_index=True, right_index=True).sort_values(by=\"Count\", ascending=False)\n",
    "\tcomparison[\"Percent\"] = comparison.apply(lambda x: x[\"Population\"]/x[\"Count\"]*100 if x[\"Count\"] > 0 else 0, axis=1)\n",
    "\tcomparison=comparison.merge(pd.DataFrame.from_dict(load_labels(f\"{root}Raw/res_classes_label.nt\"), orient=\"index\", columns=[\"Label\"]), right_index=True, left_index=True)\n",
    "\n",
    "\treturn comparison\n",
    "\n",
    "def recursivly_retrieve_data(c_initial:str, is_upper_class_of:dict, ents_per_classes:dict, already_computed:dict):\n",
    "\n",
    "\tdef inner_function(c_initial:str, is_upper_class_of:dict, ents_per_classes:dict, already_checked:set, start, already_computed:dict):\n",
    "\t\tif c_initial in already_computed:\n",
    "\t\t\treturn already_computed[c_initial]\n",
    "\t\telif c_initial in already_checked :\n",
    "\t\t\treturn  set()\n",
    "\t\telif c_initial not in is_upper_class_of:\n",
    "\t\t\treturn ents_per_classes[c_initial] if c_initial in ents_per_classes else  set()\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\tres_forward = reduce(lambda x, y: x.union(y), \n",
    "\t\t\t\t\t\t\t\t\t\t[inner_function(c_initial=c, \n",
    "\t\t\t\t\t\t\t\t\t\tis_upper_class_of=is_upper_class_of,\n",
    "\t\t\t\t\t\t\t\t\t\tents_per_classes=ents_per_classes,\n",
    "\t\t\t\t\t\t\t\t\t\talready_checked=already_checked.union({c_initial}, start),\n",
    "\t\t\t\t\t\t\t\t\t\tstart=start,\n",
    "\t\t\t\t\t\t\t\t\t\talready_computed=already_computed) for c in is_upper_class_of[c_initial]])\n",
    "\t\t\tif c_initial in ents_per_classes:\n",
    "\t\t\t\talready_computed[c_initial] = ents_per_classes[c_initial].union(res_forward)\n",
    "\t\t\telse:\n",
    "\t\t\t\talready_computed[c_initial] =  res_forward\n",
    "\t\t\treturn already_computed[c_initial]\n",
    "\n",
    "\treturn inner_function(c_initial=c_initial, is_upper_class_of=is_upper_class_of, ents_per_classes=ents_per_classes, already_checked=set(), start=c_initial, already_computed=already_computed)\n",
    "\n",
    "def return_split_graph(graph:set):\n",
    "\tdp, op = set(), set()\n",
    "\tfor line in graph:\n",
    "\t\tis_op = line[4]\n",
    "\t\tif is_op:\n",
    "\t\t\top.add(line)\n",
    "\t\telse:\n",
    "\t\t\tdp.add(line)\n",
    "\treturn dp, op\n",
    "\n",
    "def create_folder(directory:str):\n",
    "\tif not os.path.exists(directory):\n",
    "\t\tos.makedirs(directory)\n",
    "\n",
    "def create_indexes(graph:set):\n",
    "\tcpt_entity = 0 \n",
    "\tindex_entity = dict()\n",
    "\n",
    "\tcpt_relation = 0 \n",
    "\tindex_relation = dict()\n",
    "\n",
    "\tfor line in graph:\n",
    "\t\th, r, o, time, is_object, is_quad = line\n",
    "\n",
    "\t\th = h.split(\"/\")[-1][:-1]\n",
    "\t\tif h not in index_entity:\n",
    "\t\t\tindex_entity[h] = cpt_entity\n",
    "\t\t\tcpt_entity += 1 \n",
    "\t\t\n",
    "\t\tr = r.split(\"/\")[-1][:-1]\n",
    "\t\tif r not in index_relation:\n",
    "\t\t\tindex_relation[r] = cpt_relation\n",
    "\t\t\tcpt_relation += 1 \n",
    "\n",
    "\t\tif is_object:\n",
    "\t\t\to = o.split(\"/\")[-1][:-1]\n",
    "\t\t\tif o not in index_entity:\n",
    "\t\t\t\tindex_entity[o] = cpt_entity\n",
    "\t\t\t\tcpt_entity += 1\n",
    "\n",
    "\treturn index_entity, index_relation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16682/3433810285.py:4: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  return np.datetime64(time_raw, time_precision)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number removed : 260698\n",
      "Number removed : 767659\n"
     ]
    }
   ],
   "source": [
    "g_quad = read_quad(f\"{root}Raw/result_timestamps.quad\", time_precision=TEMPORAL_PRECISION,\n",
    "\t\t\t\t   threshold_start=TEMPORAL_LIMIT_START, threshold_end=TEMPORAL_LIMIT_END)\n",
    "g_quint = read_quint(f\"{root}Raw/result_intervals.quintuplet\", time_precision=TEMPORAL_PRECISION,\n",
    "\t\t\t\t   threshold_start=TEMPORAL_LIMIT_START, threshold_end=TEMPORAL_LIMIT_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entities = return_entities([g_quad, g_quint])\n",
    "\n",
    "#classes_per_ents = dict()\n",
    "#ents_per_classes = dict()\n",
    "#with open(f\"{root}Raw/res_classes.nt\", \"r\", encoding=\"UTF-8\") as f_r:\n",
    "#\tline = f_r.readline()\n",
    "\n",
    "#\twhile line != \"\":\n",
    "\t\t\n",
    "#\t\ts,_,o = line[:-2].split(\"\\t\")\n",
    "#\t\ts = s[1:-1].split(\"/\")[-1]\n",
    "#\t\to = o[1:-1].split(\"/\")[-1]\n",
    "\n",
    "#\t\tif s in entities:\n",
    "#\t\t\tif s not in classes_per_ents:\n",
    "#\t\t\t\tclasses_per_ents[s] = set()\n",
    "#\t\t\tclasses_per_ents[s].add(o)\n",
    "\n",
    "#\t\t\tif o not in ents_per_classes:\n",
    "#\t\t\t\tents_per_classes[o] = set()\n",
    "#\t\t\tents_per_classes[o].add(s)\n",
    "\n",
    "#\t\tline = f_r.readline()\n",
    "\t\n",
    "#del entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_upper_class_of = dict()\n",
    "#with open(f\"{root}Raw/Hierarchy_classes.nt\", \"r\", encoding=\"UTF-8\") as f_r:\n",
    "#\tline = f_r.readline()\n",
    "#\twhile line != \"\":\n",
    "#\t\ts,_,o = line[:-2].split(\"\\t\")\n",
    "#\t\ts = s[:-1].split(\"/\")[-1]\n",
    "#\t\to = o[:-1].split(\"/\")[-1]\n",
    "#\t\tif s[0]==\"Q\" and o[0]==\"Q\":\n",
    "#\t\t\tif s not in is_upper_class_of:\n",
    "#\t\t\t\tis_upper_class_of[s] = set()\n",
    "#\t\t\tis_upper_class_of[s].add(o)\n",
    "#\t\tline = f_r.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elts_per_c = dict()\n",
    "#for c in tqdm(list(ents_per_classes.keys())):\n",
    "#\telts_per_c[c] = recursivly_retrieve_data(c, is_upper_class_of=is_upper_class_of, ents_per_classes=ents_per_classes, already_computed=elts_per_c)\n",
    "\n",
    "\n",
    "#ids = {}\n",
    "#with open(\"./SaveRecursion/ids\", \"w\", encoding=\"UTF-8\") as f_w:\n",
    "\n",
    "#\tfor i, e in enumerate(classes_per_ents):\n",
    "#\t\tf_w.write(f\"{i}\\t{e}\\n\")\n",
    "#\t\tids[e] = i\n",
    "\n",
    "#with open(\"./SaveRecursion/res_recursion\", \"w\", encoding=\"UTF-8\") as f_w:\n",
    "#\tfor c in elts_per_c:\n",
    "#\t\tf_w.write(f\"{c}\\t\")\n",
    "#\t\tfor e in elts_per_c[c]:\n",
    "#\t\t\tf_w.write(f\"{ids[e]} \")\n",
    "#\t\tf_w.write(\"\\n\")\n",
    "\n",
    "#del elts_per_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the temp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_dp, ex_op = return_split_graph(g_quad.union(g_quint))\n",
    "len(ex_dp), len(ex_op)\n",
    "\n",
    "create_folder(f\"{root}{sub_folder}\")\n",
    "\n",
    "with open(f\"{root}{sub_folder}metadata.txt\", \"w\", encoding=\"UTF-8\") as f_w:\n",
    "\tf_w.write(f\"Sampling method : Uniform_sampling_abs\\n\")\n",
    "\tf_w.write(f\"Temporal granularity : {TEMPORAL_PRECISION}\\n\")\n",
    "\tf_w.write(f\"Temporal limit START : {TEMPORAL_LIMIT_START}\\n\")\n",
    "\tf_w.write(f\"Temporal limit END : {TEMPORAL_LIMIT_END}\\n\")\n",
    "\tf_w.write(f\"Number timestamped facts : {len(g_quad)}\\n\")\n",
    "\tf_w.write(f\"Number interval facts : {len(g_quint)}\\n\")\n",
    "\n",
    "create_folder(f\"{root}{sub_folder}Temporary/\")\n",
    "\n",
    "with open(f\"{root}{sub_folder}Temporary/OP_prepro.nt\", \"w\", encoding=\"UTF-8\") as f_w:\n",
    "\tfor line in ex_op:\n",
    "\t\th, r, o, time, _, is_quad = line \n",
    "\t\th = h.split(\"/\")[-1][:-1]\n",
    "\t\tr = r.split(\"/\")[-1][:-1]\n",
    "\t\to = o.split(\"/\")[-1][:-1]\n",
    "\t\tif is_quad ==False:\n",
    "\t\t\tf_w.write(f\"{h}\\t{r}\\t{o}\\t{time[0]}\\t{time[1]}.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tf_w.write(f\"{h}\\t{r}\\t{o}\\t{time}.\\n\")\n",
    "\n",
    "with open(f\"{root}{sub_folder}Temporary/DP_prepro.nt\", \"w\", encoding=\"UTF-8\") as f_w:\n",
    "\tfor line in ex_dp:\n",
    "\t\th, r, o, time, _, is_quad = line \n",
    "\t\th = h.split(\"/\")[-1][:-1]\n",
    "\t\tr = r.split(\"/\")[-1][:-1]\n",
    "\t\tif is_quad ==False:\n",
    "\t\t\tf_w.write(f'{h}\\t{r}\\t{o}\\t{time[0]}\\t{time[1]}.\\n')\n",
    "\t\telse:\n",
    "\t\t\tf_w.write(f'{h}\\t{r}\\t{o}\\t{time}.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MichaelConda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
