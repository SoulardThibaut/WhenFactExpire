{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64436b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8228f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44925"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT = \"./../Data/\"\n",
    "\n",
    "\n",
    "(np.datetime64(\"2023-01-01\", \"D\") - np.datetime64(\"1900-01-01\", \"D\")).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a567c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1c30eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR&  Y&Extra Small&1 189&440&51&40&283&126&142\\\\\n",
      "TR&  Y&Small&30 169&35 925&17&227&1 344&126&4 293\\\\\n",
      "TR&  Y&Medium&57 951&70 643&16&255&2 263&126&8 208\\\\\n",
      "TR&  Y&Large&164 461&280 905&12&366&4 477&126&22 923\\\\\n",
      "TR&  D&Extra Small&1 105&443&52&41&279&45 072&0\\\\\n",
      "TR&  D&Small&33 318&37 898&18&229&1 452&45 072&13\\\\\n",
      "TR&  D&Medium&64 201&76 645&17&259&2 494&45 072&26\\\\\n",
      "TR&  D&Large&189 851&316 371&12&372&5 079&45 072&75\\\\\n",
      "\\hline\n",
      "\\hline\n",
      "FD&  Y&Extra Small&1 303&461&52&40&298&3 026&6\\\\\n",
      "FD&  Y&Small&34 027&39 675&17&245&1 383&3 026&199\\\\\n",
      "FD&  Y&Medium&63 192&77 210&16&279&2 273&3 026&376\\\\\n",
      "FD&  Y&Large&182 383&314 111&12&408&4 492&3 026&1 072\\\\\n",
      "FD&  D&Extra Small&1 151&464&52&41&295&1 104 275&0\\\\\n",
      "FD&  D&Small&36 281&41 642&18&246&1 488&1 104 275&1\\\\\n",
      "FD&  D&Medium&70 572&83 203&17&282&2 491&1 104 275&1\\\\\n",
      "FD&  D&Large&207 823&349 995&12&411&5 073&1 104 275&3\\\\\n",
      "\\hline\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "names = [(\"Large\", 4), (\"Medium\", 8), (\"Small\", 10), (\"Extra_Small\", 15)]\n",
    "\n",
    "stats = dict()\n",
    "\n",
    "def compute_possible_timestamps(type:str, granularity:str):\n",
    "\tpossible = dict()\n",
    "\tif type == \"TR\":\n",
    "\t\tfor i in range((np.datetime64(\"2023-05-25\", granularity) - np.datetime64(\"1900-01-01\", granularity)).astype(int)+2):\n",
    "\t\t\tpossible[str(np.datetime64(\"1900-01-01\", granularity)+i)] = 0\n",
    "\telse:\n",
    "\t\tfor i in range((np.datetime64(\"2023-05-25\", granularity) - np.datetime64(\"-1000-01-01\", granularity)).astype(int)+2):\n",
    "\t\t\tpossible[str(np.datetime64(\"-1000-01-01\", granularity)+i)] = 0\n",
    "\n",
    "\n",
    "\tpossible[\"None\"] = 0\n",
    "\treturn possible\n",
    "\n",
    "for temporal_precision in [\"Y\", \"D\"]:\n",
    "\tfor type_data in [\"TR\", \"FD\"]:\n",
    "\n",
    "\t\tfor name, limit in names:\n",
    "\t\t\tsub_folder = f\"{ROOT}{type_data}_{temporal_precision}/{name}/\"\n",
    "\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"] = {}\n",
    "\n",
    "\t\t\tusage_entity = dict()\n",
    "\t\t\tusage_relation = dict()\n",
    "\t\t\tusage_timestamp = compute_possible_timestamps(type_data, temporal_precision)\n",
    "\t\t\twith open(f\"{sub_folder}OP_train.txt\") as f_r:\n",
    "\t\t\t\tline = f_r.readline()\n",
    "\t\t\t\tcpt = 0\n",
    "\t\t\t\twhile line != \"\":\n",
    "\t\t\t\t\tcpt +=1\n",
    "\t\t\t\t\telts = line[:-1].split(\"\\t\")\n",
    "\t\t\t\t\tif elts[0] not in usage_entity:\n",
    "\t\t\t\t\t\tusage_entity[elts[0]] = 0\n",
    "\t\t\t\t\tusage_entity[elts[0]] += 1\n",
    "\n",
    "\t\t\t\t\tif elts[1] not in usage_relation:\n",
    "\t\t\t\t\t\tusage_relation[elts[1]] = 0\n",
    "\t\t\t\t\tusage_relation[elts[1]] += 1\n",
    "\n",
    "\t\t\t\t\tif elts[2] not in usage_entity:\n",
    "\t\t\t\t\t\tusage_entity[elts[2]] = 0\n",
    "\t\t\t\t\tusage_entity[elts[2]] += 1\n",
    "\n",
    "\t\t\t\t\t#if elts[3] not in usage_timestamp:\n",
    "\t\t\t\t\t#\tusage_timestamp[elts[3]] = 0\n",
    "\t\t\t\t\tusage_timestamp[elts[3]] += 1\n",
    "\n",
    "\t\t\t\t\tif len(elts) == 5:\n",
    "\n",
    "\t\t\t\t\t\t#if elts[4] not in usage_timestamp:\n",
    "\t\t\t\t\t\t#\tusage_timestamp[elts[4]] = 0\n",
    "\t\t\t\t\t\tusage_timestamp[elts[4]] += 1\n",
    "\n",
    "\t\t\t\t\tline = f_r.readline()\n",
    "\t\t\t\t\tline = f_r.readline()\n",
    "\n",
    "\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Size\"] = cpt\n",
    "\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Ents\"] = len(usage_entity)\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Relations\"] = len(usage_relation)\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Timestamps\"] = len(usage_timestamp)\n",
    "\n",
    "\t\t\t\t#print(temporal_precision, type_data, name, np.average(list(usage_entity.values())))\n",
    "\t\t\t\t#print(np.quantile(list(usage_entity.values()), [0.2,.4,.6,.8, .9, 1]))\n",
    "\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Ents\"] = np.around(np.average(list(usage_entity.values())),0)\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Relations\"] = np.around(np.average(list(usage_relation.values())),0)\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Timestamps\"] = np.around(np.average(list(usage_timestamp.values())),0)\n",
    "\n",
    "\n",
    "\t\t\tfor file in [\"OP_valid.txt\", \"OP_test.txt\"]:\n",
    "\t\t\t\twith open(f\"{sub_folder}{file}\") as f_r:\n",
    "\t\t\t\t\tline = f_r.readline()\n",
    "\t\t\t\t\tcpt = 0\n",
    "\t\t\t\t\twhile line != \"\":\n",
    "\t\t\t\t\t\tcpt +=1\n",
    "\t\t\t\t\t\telts = line[:-1].split(\"\\t\")\n",
    "\t\t\t\t\t\tif elts[0] not in usage_entity:\n",
    "\t\t\t\t\t\t\tusage_entity[elts[0]] = 0\n",
    "\t\t\t\t\t\tusage_entity[elts[0]] += 1\n",
    "\n",
    "\t\t\t\t\t\tif elts[1] not in usage_relation:\n",
    "\t\t\t\t\t\t\tusage_relation[elts[1]] = 0\n",
    "\t\t\t\t\t\tusage_relation[elts[1]] += 1\n",
    "\n",
    "\t\t\t\t\t\tif elts[2] not in usage_entity:\n",
    "\t\t\t\t\t\t\tusage_entity[elts[2]] = 0\n",
    "\t\t\t\t\t\tusage_entity[elts[2]] += 1\n",
    "\n",
    "\t\t\t\t\t\t#if elts[3] not in usage_timestamp:\n",
    "\t\t\t\t\t\t#\tusage_timestamp[elts[3]] = 0\n",
    "\t\t\t\t\t\tusage_timestamp[elts[3]] += 1\n",
    "\n",
    "\t\t\t\t\t\tif len(elts) == 5:\n",
    "\n",
    "\t\t\t\t\t\t\t#if elts[4] not in usage_timestamp:\n",
    "\t\t\t\t\t\t\t#\tusage_timestamp[elts[4]] = 0\n",
    "\t\t\t\t\t\t\tusage_timestamp[elts[4]] += 1\n",
    "\n",
    "\t\t\t\t\t\tline = f_r.readline()\n",
    "\t\t\t\t\t\tline = f_r.readline()\n",
    "\n",
    "\n",
    "\t\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Size\"] = cpt\n",
    "\n",
    "\t\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Ents\"] = len(usage_entity)\n",
    "\t\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Relations\"] = len(usage_relation)\n",
    "\t\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Timestamps\"] = len(usage_timestamp)\n",
    "\n",
    "\t\t\t\t\t#print(temporal_precision, type_data, name, np.average(list(usage_entity.values())))\n",
    "\t\t\t\t\t#print(np.quantile(list(usage_entity.values()), [0.2,.4,.6,.8, .9, 1]))\n",
    "\n",
    "\t\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Ents\"] = np.around(np.average(list(usage_entity.values())),0)\n",
    "\t\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Relations\"] = np.around(np.average(list(usage_relation.values())),0)\n",
    "\t\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Timestamps\"] = np.around(np.average(list(usage_timestamp.values())),0)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t#with open(f\"{sub_folder}OP_reduced_{limit}.nt\") as f_r:\n",
    "\t\t\t#\tline = f_r.readline()\n",
    "\t\t\t#\twhile line != \"\":\n",
    "\t\t\t#\t\tcpt+=1\n",
    "\t\t\t#\t\telts = line[:-2].split(\"\\t\")\n",
    "\t\t\t#\t\tif elts[0] not in usage_entity:\n",
    "\t\t\t#\t\t\tusage_entity[elts[0]] = 0\n",
    "\t\t\t#\t\tusage_entity[elts[0]] += 1\n",
    "\n",
    "\t\t\t#\t\tif elts[1] not in usage_relation:\n",
    "\t\t\t#\t\t\tusage_relation[elts[1]] = 0\n",
    "\t\t\t#\t\tusage_relation[elts[1]] += 1\n",
    "\n",
    "\t\t\t#\t\tif elts[2] not in usage_entity:\n",
    "\t\t\t#\t\t\tusage_entity[elts[2]] = 0\n",
    "\t\t\t#\t\tusage_entity[elts[2]] += 1\n",
    "\n",
    "\t\t\t#\t\t#if elts[3] not in usage_timestamp:\n",
    "\t\t\t#\t\t#\tusage_timestamp[elts[3]] = 0\n",
    "\t\t\t#\t\tusage_timestamp[elts[3]] += 1\n",
    "\n",
    "\t\t\t#\t\tif len(elts) == 5:\n",
    "\n",
    "\t\t\t#\t\t\t#if elts[4] not in usage_timestamp:\n",
    "\t\t\t#\t\t\t#\tusage_timestamp[elts[4]] = 0\n",
    "\t\t\t#\t\t\tusage_timestamp[elts[4]] += 1\n",
    "\n",
    "\t\t\t#\t\tline = f_r.readline()\n",
    "\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Size +DP\"] = cpt\n",
    "\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Relations +DP\"] = len(usage_relation)\n",
    "\t\t\t\t\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Ents +DP\"] = np.around(np.average(list(usage_entity.values())),0)\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Relations +DP\"] = np.around(np.average(list(usage_relation.values())),0)\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Timestamps +DP\"] = np.around(np.average(list(usage_timestamp.values())),0)\n",
    "\n",
    "\n",
    "a=pd.DataFrame.from_dict(stats, orient=\"index\")\n",
    "for scope in [\"TR\", \"FD\"]:\n",
    "\tfor granularity in [\"Y\", \"D\"]:\n",
    "\t\tfor size in [\"Extra_Small\", \"Small\", \"Medium\", \"Large\"]:\n",
    "\t\t\tstat_sub_data = stats[f\"{scope}_{granularity}_{size}\"]\n",
    "\t\t\t\n",
    "\t\t\tto_print = f\"{scope}&  {granularity}&{size.replace('_', ' ')}&\"\n",
    "\t\t\tto_print += \"{:,}\".format(stat_sub_data[\"Size\"]).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(stat_sub_data[\"# Ents\"]).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(int(stat_sub_data[\"Avg usage Ents\"])).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(stat_sub_data[\"# Relations\"]).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(int(stat_sub_data[\"Avg usage Relations\"])).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(stat_sub_data[\"# Timestamps\"]).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(int(stat_sub_data[\"Avg usage Timestamps\"])).replace(\",\",\" \")+\"\\\\\\\\\"\n",
    "\t\t\tprint(to_print )\n",
    "\tprint(\"\\\\hline\")\n",
    "\tprint(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9082a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR& Extra Small& Y&8 992&440&41&39&231&126&113\\\\\n",
      "TR& Extra Small& D&9 183&443&41&40&230&45 072&0\\\\\n",
      "TR& Small& Y&244 550&35 659&14&224&1 092&126&3 441\\\\\n",
      "TR& Small& D&266 191&37 720&14&216&1 232&45 072&11\\\\\n",
      "TR& Medium& Y&461 553&70 296&13&247&1 869&126&6 567\\\\\n",
      "TR& Medium& D&517 005&76 480&14&250&2 068&45 072&21\\\\\n",
      "TR& Large& Y&1 310 729&280 026&9&351&3 734&126&18 335\\\\\n",
      "TR& Large& D&1 510 565&316 185&10&363&4 161&45 072&60\\\\\n",
      "\\hline\n",
      "\\hline\n",
      "FD& Extra Small& Y&9 441&461&41&39&242&3 026&5\\\\\n",
      "FD& Extra Small& D&9 702&464&42&41&237&1 104 275&0\\\\\n",
      "FD& Small& Y&270 898&39 441&14&233&1 163&3 026&159\\\\\n",
      "FD& Small& D&293 185&41 440&14&241&1 217&1 104 275&0\\\\\n",
      "FD& Medium& Y&507 712&76 876&13&270&1 880&3 026&301\\\\\n",
      "FD& Medium& D&561 131&83 025&14&274&2 048&1 104 275&1\\\\\n",
      "FD& Large& Y&1 466 511&313 252&9&391&3 751&3 026&858\\\\\n",
      "FD& Large& D&1 668 023&349 791&10&393&4 244&1 104 275&3\\\\\n",
      "\\hline\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bac43f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR& Extra Small& Y&11 447&443&52&41&279&126&144\\\\\n",
      "TR& Extra Small& D&11 465&443&52&41&280&45 072&0\\\\\n",
      "TR& Small& Y&330 312&37 915&17&229&1 442&126&4 690\\\\\n",
      "TR& Small& D&332 966&37 913&18&229&1 454&45 072&13\\\\\n",
      "TR& Medium& Y&642 778&76 742&17&260&2 472&126&9 243\\\\\n",
      "TR& Medium& D&647 063&76 735&17&260&2 489&45 072&26\\\\\n",
      "TR& Large& Y&1 926 728&326 146&12&374&5 152&126&27 105\\\\\n",
      "TR& Large& D&1 934 088&326 113&12&374&5 171&45 072&76\\\\\n",
      "\\hline\n",
      "\\hline\n",
      "FD& Extra Small& Y&12 064&464&52&41&294&3 026&6\\\\\n",
      "FD& Extra Small& D&12 082&464&52&41&295&1 104 275&0\\\\\n",
      "FD& Small& Y&363 407&41 654&17&246&1 477&3 026&216\\\\\n",
      "FD& Small& D&366 153&41 653&18&246&1 488&1 104 275&1\\\\\n",
      "FD& Medium& Y&698 836&83 295&17&282&2 478&3 026&419\\\\\n",
      "FD& Medium& D&703 267&83 289&17&282&2 494&1 104 275&1\\\\\n",
      "FD& Large& Y&2 120 990&359 746&12&411&5 161&3 026&1 246\\\\\n",
      "FD& Large& D&2 128 925&359 714&12&411&5 180&1 104 275&3\\\\\n",
      "\\hline\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "def compute_possible_timestamps(type:str, granularity:str):\n",
    "\tpossible = dict()\n",
    "\tif type == \"TR\":\n",
    "\t\tfor i in range((np.datetime64(\"2023-05-25\", granularity) - np.datetime64(\"1900-01-01\", granularity)).astype(int)+2):\n",
    "\t\t\tpossible[str(np.datetime64(\"1900-01-01\", granularity)+i)] = 0\n",
    "\telse:\n",
    "\t\tfor i in range((np.datetime64(\"2023-05-25\", granularity) - np.datetime64(\"-1000-01-01\", granularity)).astype(int)+2):\n",
    "\t\t\tpossible[str(np.datetime64(\"-1000-01-01\", granularity)+i)] = 0\n",
    "\n",
    "\n",
    "\tpossible[\"Unknown\"] = 0\n",
    "\treturn possible\n",
    "\n",
    "statsnames = [(\"Large\", 4), (\"Medium\", 8), (\"Small\", 10), (\"Extra_Small\", 15)]\n",
    "\n",
    "stats = dict()\n",
    "\n",
    "for temporal_precision in [\"Y\", \"D\"]:\n",
    "\tfor type_data in [\"TR\", \"FD\"]:\n",
    "\t\tsub_folder = f\"{ROOT}{type_data}_{temporal_precision}/Temporary/\"\n",
    "\n",
    "\t\tfor name, limit in names:\n",
    "\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"] = {}\n",
    "\n",
    "\t\t\tusage_entity = dict()\n",
    "\t\t\tusage_relation = dict()\n",
    "\t\t\tusage_timestamp = compute_possible_timestamps(type_data, temporal_precision)\n",
    "\t\t\twith open(f\"{sub_folder}OP_reduced_{limit}.nt\") as f_r:\n",
    "\t\t\t\tline = f_r.readline()\n",
    "\t\t\t\tcpt = 0\n",
    "\t\t\t\twhile line != \"\":\n",
    "\t\t\t\t\tcpt +=1\n",
    "\t\t\t\t\telts = line[:-2].split(\"\\t\")\n",
    "\t\t\t\t\tif elts[0] not in usage_entity:\n",
    "\t\t\t\t\t\tusage_entity[elts[0]] = 0\n",
    "\t\t\t\t\tusage_entity[elts[0]] += 1\n",
    "\n",
    "\t\t\t\t\tif elts[1] not in usage_relation:\n",
    "\t\t\t\t\t\tusage_relation[elts[1]] = 0\n",
    "\t\t\t\t\tusage_relation[elts[1]] += 1\n",
    "\n",
    "\t\t\t\t\tif elts[2] not in usage_entity:\n",
    "\t\t\t\t\t\tusage_entity[elts[2]] = 0\n",
    "\t\t\t\t\tusage_entity[elts[2]] += 1\n",
    "\n",
    "\t\t\t\t\t#if elts[3] not in usage_timestamp:\n",
    "\t\t\t\t\t#\tusage_timestamp[elts[3]] = 0\n",
    "\t\t\t\t\tusage_timestamp[elts[3]] += 1\n",
    "\n",
    "\t\t\t\t\tif len(elts) == 5:\n",
    "\n",
    "\t\t\t\t\t\t#if elts[4] not in usage_timestamp:\n",
    "\t\t\t\t\t\t#\tusage_timestamp[elts[4]] = 0\n",
    "\t\t\t\t\t\tusage_timestamp[elts[4]] += 1\n",
    "\n",
    "\t\t\t\t\tline = f_r.readline()\n",
    "\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Size\"] = cpt\n",
    "\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Ents\"] = len(usage_entity)\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Relations\"] = len(usage_relation)\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Timestamps\"] = len(usage_timestamp)\n",
    "\n",
    "\t\t\t\t#print(temporal_precision, type_data, name, np.average(list(usage_entity.values())))\n",
    "\t\t\t\t#print(np.quantile(list(usage_entity.values()), [0.2,.4,.6,.8, .9, 1]))\n",
    "\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Ents\"] = np.around(np.average(list(usage_entity.values())),0)\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Relations\"] = np.around(np.average(list(usage_relation.values())),0)\n",
    "\t\t\t\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Timestamps\"] = np.around(np.average(list(usage_timestamp.values())),0)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t#with open(f\"{sub_folder}OP_reduced_{limit}.nt\") as f_r:\n",
    "\t\t\t#\tline = f_r.readline()\n",
    "\t\t\t#\twhile line != \"\":\n",
    "\t\t\t#\t\tcpt+=1\n",
    "\t\t\t#\t\telts = line[:-2].split(\"\\t\")\n",
    "\t\t\t#\t\tif elts[0] not in usage_entity:\n",
    "\t\t\t#\t\t\tusage_entity[elts[0]] = 0\n",
    "\t\t\t#\t\tusage_entity[elts[0]] += 1\n",
    "\n",
    "\t\t\t#\t\tif elts[1] not in usage_relation:\n",
    "\t\t\t#\t\t\tusage_relation[elts[1]] = 0\n",
    "\t\t\t#\t\tusage_relation[elts[1]] += 1\n",
    "\n",
    "\t\t\t#\t\tif elts[2] not in usage_entity:\n",
    "\t\t\t#\t\t\tusage_entity[elts[2]] = 0\n",
    "\t\t\t#\t\tusage_entity[elts[2]] += 1\n",
    "\n",
    "\t\t\t#\t\t#if elts[3] not in usage_timestamp:\n",
    "\t\t\t#\t\t#\tusage_timestamp[elts[3]] = 0\n",
    "\t\t\t#\t\tusage_timestamp[elts[3]] += 1\n",
    "\n",
    "\t\t\t#\t\tif len(elts) == 5:\n",
    "\n",
    "\t\t\t#\t\t\t#if elts[4] not in usage_timestamp:\n",
    "\t\t\t#\t\t\t#\tusage_timestamp[elts[4]] = 0\n",
    "\t\t\t#\t\t\tusage_timestamp[elts[4]] += 1\n",
    "\n",
    "\t\t\t#\t\tline = f_r.readline()\n",
    "\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Size +DP\"] = cpt\n",
    "\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"# Relations +DP\"] = len(usage_relation)\n",
    "\t\t\t\t\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Ents +DP\"] = np.around(np.average(list(usage_entity.values())),0)\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Relations +DP\"] = np.around(np.average(list(usage_relation.values())),0)\n",
    "\t\t\t#\tstats[f\"{type_data}_{temporal_precision}_{name}\"][\"Avg usage Timestamps +DP\"] = np.around(np.average(list(usage_timestamp.values())),0)\n",
    "\n",
    "a=pd.DataFrame.from_dict(stats, orient=\"index\")\n",
    "for scope in [\"TR\", \"FD\"]:\n",
    "\tfor size in [\"Extra_Small\", \"Small\", \"Medium\", \"Large\"]:\n",
    "\t\tfor granularity in [\"Y\", \"D\"]:\n",
    "\t\t\tstat_sub_data = stats[f\"{scope}_{granularity}_{size}\"]\n",
    "\t\t\t\n",
    "\t\t\tto_print = f\"{scope}& {size.replace('_', ' ')}& {granularity}&\"\n",
    "\t\t\tto_print += \"{:,}\".format(stat_sub_data[\"Size\"]).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(stat_sub_data[\"# Ents\"]).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(int(stat_sub_data[\"Avg usage Ents\"])).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(stat_sub_data[\"# Relations\"]).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(int(stat_sub_data[\"Avg usage Relations\"])).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(stat_sub_data[\"# Timestamps\"]).replace(\",\",\" \")+\"&\"\n",
    "\t\t\tto_print += \"{:,}\".format(int(stat_sub_data[\"Avg usage Timestamps\"])).replace(\",\",\" \")+\"\\\\\\\\\"\n",
    "\t\t\tprint(to_print )\n",
    "\tprint(\"\\\\hline\")\n",
    "\tprint(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f935b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MichaelConda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
